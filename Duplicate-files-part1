#include <list>
#include <string>
#include <queue>
#include <unordered_map>
using namespace std;

// You may assume that list_dir, is_dir, join_path are implemented.
auto list_dir(const string& path) {
  /*
    Returns a non-recursive list of entries (files and folders) with relative
    paths in this directory only. Basically the same functionality as doing "ls"
    or "dir" in a shell.
  */
  return list<string>();
}

auto is_dir(const string& path) {
  // Returns True if the given path corresponds to a directory.
  return false;
}

auto join_path(const string& path, const string& subpath) {
  // Returns a path formed by combining the given path and the given subpath.
  return string();
}

list<list<string>> find_duplicates_optimized(const string& root_path) {
    list<list<string>> result;
    
    if(root_path.empty())
        return result;

    queue<string> Q; //For BFS walk of the files
    unordered_map<int, list<string>> files_by_size;
    Q.push(root_path);
    while(!Q.empty()) {
        string dir = Q.front();
        Q.pop();
        list<string> dir_contents = list_dir(dir);
        for(auto entry : dir_contents) {
            string abs_path = join_path(dir, entry); //Get the absolute path of the file/dir
            if(is_dir(entry)) {
                Q.push(abs_path);
            } else {
                //Entry is a file. Get size of the file and insert into map
                int fsize = get_file_size(abs_path); //Get file size
                if(fsize > 0) //Files having 0 size does not have any content to compare, unless we consider all the 0-size file as same
                    files_by_size[fsize].insert(abs_path);
            }
        }
    }

    unordered_map<string, list<string>> dup_files; //For file_hash->list of files
    //Get list of same files from each entry in the files_by_size and insert into result list
    for(auto iter:files_by_size) {
        if(iter.second.size() > 1) { //List having multiple files of same size
            list<string> files = iter.second;
            //Iter through files, compute the hash and insert into map.
            for(auto file: files) {
                string hash = compute_hash(file);
                dup_files[hash].insert(file);
            }
        }
    }
    
    //Iterate through map to find all the entries having value with multiple entries in list
    for(auto iter:dup_files) {
        if(iter.second.size() > 1) {
            result.insert(iter.second);
        }
    }
    return result;
}

list<list<string>> find_duplicates(const string& root_path) {
    list<list<string>> result;
    if(root_path.empty())
        return result;
    queue<string> Q; //For BFS walk of the files
    unordered_map<string, list<string>> dup_files; //For file_hash->list of files
    Q.push(root_path);
    while(!Q.empty()) {
        string dir = Q.front();
        Q.pop();
        list<string> dir_contents = list_dir(dir);
        for(auto entry : dir_contents) {
            string abs_path = join_path(dir, entry); //Get the absolute path of the file/dir
            if(is_dir(entry)) {
                Q.push(abs_path);
            } else {
                //Entry is a file. Compute the hash of the file and insert into map
                string hash = compute_hash(abs_path);
                dup_files[hash].insert(abs_path);
            }
        }
    }
    //Iterate through map to find all the entries having value with multiple entries in list
    for(auto iter:dup_files) {
        if(iter.second.size() > 1) {
            result.insert(iter.second);
        }
    }
    return result;
}

int main() {
  auto results = find_duplicates("/foo/bar");
}
